rm(list=ls())

library(psych)
library(dplyr)
library(gsheet)
library(ggplot2)
library(lm.beta)
library(car)
library(rgl)


#importing data
data_sample_1 = read.csv("https://raw.githubusercontent.com/kekecsz/PSYP13_Data_analysis_class-2018/master/home_sample_1.csv")


data = data_sample_1



# View data in the data viewer tool 
View(data)

# or display simple descriptive statistics and plots
describe(data$age)


# histograms
hist(data$age, breaks = 20)


#fixing outliers
data = data[-which(data[,"age"] == "222"),] #person ID_28 was 222 years old
data = data[-which(data[,"ID"] == "ID_112"),] #mindfullness score below possible
data = data[-which(data[,"ID"] == "ID_146"),] #mindfullness score below possible 


#checking STAI_trait
describe(data$STAI_trait)
hist(data$STAI_trait, breaks = 20)


#checking pain_cat
describe(data$pain_cat)
hist(data$pain_cat, breaks = 20)

#checking mindfulness
describe(data$mindfulness)
hist(data$mindfulness, breaks = 20)

#checking cortisol_serum
describe(data$cortisol_serum)
hist(data$cortisol_serum, breaks = 20)

#REMOVED BECAUSE OF VIF #checking cortisol_saliva
#describe(data$cortisol_saliva)
#hist(data$cortisol_saliva, breaks = 20)


###########################################################
#                                                         #
#                   MODEL 1                               #
#                                                         #
###########################################################

# Regression model for pain, with age and sex as predictors
mod1 = lm(pain ~ age + sex, data = data)

# visualize the regression equation in multiple regression
plot(pain ~ age, data = data)
abline(lm(pain ~ age, data = data))
plot(pain ~ grade, sex = data)
abline(lm(pain ~ sex, data = data))

#summary
summary(mod1)

lm.beta(mod1)

#checking linearity
plot(pain ~ age, data = data) 

plot(pain ~ sex, data = data) # categorical variable 

abline(lm(pain ~ age, data = data))


#Cook's distance for Model 1
plot(mod1, which = 5)

4/nrow(data)
plot(mod1, which = 4) 

lm.beta(mod1)

confint(mod1)

###########################################################
#                                                         #
#                   MODEL 2                               #
#                                                         #
###########################################################

# Regression model for pain, with age + sex + STAI + pain catastrophizing + mindfulness + cortisol serum (CORTISOL SALIVA REMOVED) as predictors
mod2 = lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum, data = data)

summary(mod2)

plot(pain ~ age, data = data)
abline(lm(pain ~ age, data = data))

plot(pain ~ sex, data = data) # categorical variable 

plot(pain ~ STAI_trait, data = data)
abline(lm(pain ~ STAI_trait, data = data))

plot(pain ~ pain_cat, data = data)
abline(lm(pain ~ pain_cat, data = data))

plot(pain ~ mindfulness, data = data)
abline(lm(pain ~ mindfulness, data = data))

plot(pain ~ cortisol_serum, data = data)
abline(lm(pain ~ cortisol_serum, data = data))

#REMOVED BECAUSE OF VIF #plot(pain ~ cortisol_saliva, data = data)
#REMOVED BECAUSE OF VIF #abline(lm(pain ~ cortisol_saliva, data = data))


#Cook's distance
plot(mod2, which = 5)

4/nrow(data)
plot(mod2, which = 4) 


summary(mod2)

lm.beta(mod2)
###########################################################
#                                                         #
#                  COMPARING MODELS                       #
#                                                         #
###########################################################

#running ANOVA for mod1 & mod2

anova(mod1, mod2)

#######Assumptions of linear regression

## Normality

# QQ plot
plot(mod2, which = 2) #outliers seem to be 100, 56 & 24. Not removing them since the histogram and skew&kurtosis seem within the assumption of normality 

# histogram
hist(residuals(mod2), breaks = 20) #roughly bell shaped so acceptable


# skew and kurtosis
describe(residuals(mod2)) #skewness and kurtosis not greater than 1, indicating no violation of the assumption of normality 


######QQ Plot- removing the outliers in it to just check if it makes a difference

data_nooutliers_QQ = data[-c(100, 56, 24),]

mod3 = lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + cortisol_saliva, data = data_nooutliers_QQ)

# recheck the assumption of normality of residuals
describe(residuals(mod3))
plot(mod3, which = 2)
hist(residuals(mod3), breaks = 20)

# comparing the models on data with and without the outliers
summary(mod2)
summary(mod3)

#difference between the two models is of an adjusted r squared of a mere 0.0231, which indicates that removing the outliers doesn't make a better model.

#USING MOD2 FOR THE FUTURE TESTS OF ASSUMPTIONS

###### Linearity

residualPlots(mod2) #all lines seem relatively flat, the tests are all non significant, so the linearity assumption seems to hold true for our model.


###### Homoscedasticty

plot(mod2, which = 3)
ncvTest(mod2) # NCV test #p value is greater than 0.05, which means that there is NO violation of the assumption of homoscedasticity


#installing lmtest package needed for Breush-Pagan test

library(lmtest)

bptest(mod2) # Breush-Pagan test #p value is greater than 0.05, which means that there is NO violation of the assumption of homoscedasticity

###### Multicollinearity

vif(mod2) #VIF is above 3 for cortisol serum and cortisol saliva
#there is data multicollinearity, since cortisol serum and cortisol saliva are so closely correlated
#solution is to remove cortisol saliva since cortisol serum is generally considered more indicative of stress

AIC(mod1) #checking AICs
AIC(mod2)


if(interactive())
  curve(ptukey(x, nm = 6, df = 5), from = -1, to = 8, n = 101)
(ptt <- ptukey(0:10, 2, df =  5))
(qtt <- qtukey(.95, 2, df =  2:11))

TukeyHSD(mod2)
#running tukey test
residualPlots(mod2)

residualPlots(mod1)

###########################################################
#                                                         #
#                  PART TWO                               #
#                                                         #
###########################################################

mod4_assignment2 = lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + weight, data = data)

mod_backwards = step(mod4_assignment2, direction = "backward")

backward_model = lm(pain ~ age + pain_cat + mindfulness + cortisol_serum, data = data)

theorybased_model = lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum, data = data)

summary(mod4_assignment2)

lm.beta(mod4_assignment2)

summary(backward_model)
lm.beta(backward_model)

AIC(mod4_assignment2)
AIC(backward_model)

anova(mod4_assignment2, backward_model)

anova(theorybased_model, backward_model)

summary(theorybased_model)

summary(backward_model)

summary(theorybased_model)$adj.r.squared #BETTER R SQUARED FOR THIS ONE WOOHOO 

summary(backward_model)$adj.r.squared

AIC(theorybased_model)

AIC(backward_model) #AIC lower, hence better

anova(mod4_assignment2, backward_model)

AIC(mod4_assignment2)

AIC(backward_model)

###########################################################
#                                                         #
#                TESTING MODELS ON NEW DATA               #
#                                                         #
###########################################################

data_sample_2 = read.csv("https://raw.githubusercontent.com/kekecsz/PSYP13_Data_analysis_class-2018/master/home_sample_2.csv")

#fixing outliers
data_sample_2 = data_sample_2[-which(data[,"ID"] == "ID_123"),] #mindfullness score below possible
data_sample_2 = data_sample_2[-which(data[,"ID"] == "ID_113"),] #mindfullness score below possible 
data_sample_2 = data_sample_2[-which(data[,"ID"] == "ID_43"),] #very overweight compared to others 

#lev = hat(model.matrix(theorybased_model)) #mahadaboi test
#plot(lev)
#data_sample_2[lev > .10,]
#N = nrow(data_sample_2)
#mahad=(n - 1)*(lev-1/N)
#tail(sort(mahad),5)
#order(mahad,decreasing=T)[c(5,4,3,2,1)]
#
#cook's test for outliers?
plot(theorybased_model, which = 5)
4/nrow(data_sample_2)
plot(theorybased_model, which = 4)

pred_test <- predict (theorybased_model, data_sample_2)
pred_test_backward <- predict (backward_model, data_sample_2)

RSS_test = sum((data_sample_2[,"pain"] - pred_test)^2)
RSS_test_back = sum((data_sample_2[,"pain"] - pred_test_backward)^2)
RSS_test  #results are higher which means theory regression model has more error 

RSS_test_back 



#########PART THREE


data_sample_3 = read.csv("https://raw.githubusercontent.com/kekecsz/PSYP13_Data_analysis_class-2018/master/home_sample_3.csv")

library(psych) # for describe
library(reshape2) # for melt function
library(ggplot2) # for ggplot
library(cAIC4) # for cAIC
library(r2glmm) # for r2beta

#descriptives
describe(data_sample_3)


hist(data_sample_3$pain1)
hist(data_sample_3$pain2)
hist(data_sample_3$pain3)
hist(data_sample_3$pain4)



#correlation
repeated_variables = c("pain1", "pain2", "pain3",	"pain4")
cor(data_sample_3[,repeated_variables])

#reframing data
data_sample_3_long = melt(data_sample_3, measure.vars=repeated_variables, variable.name = "time", value.name = "pain")


# order data frame by participant ID(not necessary, just makes the dataframe look more intuitive)
data_sample_3_long = data_sample_3_long[order(data_sample_3_long[,"ID"]),]

# change the time variable to a numerical vector
data_sample_3_long$time = as.numeric(data_sample_3_long$time)


View(data_sample_3_long)


#building the two models (slope and intercept)

mod_rep_int = lmer(pain ~ time + age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + (1|ID), data = data_sample_3_long)
mod_rep_slope = lmer(pain ~ time + age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + (time|ID), data = data_sample_3_long)

data_sample_3_long_withpreds = data_sample_3_long

data_sample_3_long_withpreds$pred_int = predict(mod_rep_int)
data_sample_3_long_withpreds$pred_slope = predict(mod_rep_slope)

# random intercept model
ggplot(data_sample_3_long_withpreds, aes(y = pain, x = time, group = ID))+
  geom_point(size = 3)+
  geom_line(color='red', aes(y=pred_int, x=time))+
  facet_wrap( ~ ID, ncol = 5)

# random slope model
ggplot(data_sample_3_long_withpreds, aes(y = pain, x = time, group = ID))+
  geom_point(size = 3)+
  geom_line(color='red', aes(y=pred_slope, x=time))+
  facet_wrap( ~ ID, ncol = 5)
 
#comparing the two models
cAIC(mod_rep_int)$caic
cAIC(mod_rep_slope)$caic

anova(mod_rep_int, mod_rep_slope) #going with random intercept

r2beta(mod_rep_slope)

#Adding the quadratic term of time to the model random intercept model to account for the non-linear relationship.

mod_rep_int_quad = lmer(pain ~ time + age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + I(time^2) + (1|ID), data = data_sample_3_long)


# And add the predictions to the new dataframe containing the other predicted values as well.
data_sample_3_long_withpreds$pred_int_quad = predict(mod_rep_int_quad)



# Now we can compare the model fit of the random intercept model containing the quadratic effect of time with the random intercept model without it. As usual, we use visualization, cAIC and the likelihood ratio test.

data_sample_3_long_withpreds$pred_int_quad = predict(mod_rep_int_quad)

plot_quad = ggplot(data_sample_3_long_withpreds, aes(y = pain, x = time, group = ID))+
  geom_point(size = 3)+
  geom_line(color='red', aes(y=pred_int_quad, x=time))+
  facet_wrap( ~ ID, ncol = 5)


plot_quad

cAIC(mod_rep_int)$caic
cAIC(mod_rep_int_quad)$caic

anova(mod_rep_int, mod_rep_int_quad)
##AIC lower for mod_rep_int_quad, so that one is a better fit

################################################
#IF I TOOK Slope instead of int because slope was a better fit
mod_rep_slope_quad = lmer(pain ~ time + age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + I(time^2) + (1|ID), data = data_sample_3_long)
data_sample_3_long_withpreds$pred_int_slope = predict(mod_rep_slope_quad)

plot_quad = ggplot(data_sample_3_long_withpreds, aes(y = pain, x = time, group = ID))+
  geom_point(size = 3)+
  geom_line(color='red', aes(y=pred_int_quad, x=time))+
  facet_wrap( ~ ID, ncol = 5)


plot_quad

cAIC(mod_rep_int)$caic
cAIC(mod_rep_int_quad)$caic

anova(mod_rep_slope, mod_rep_slope_quad)
##AIC lower for mod_rep_slope, so that one is a better fit


#################################################

data_sample_3_long_centered_time = data_sample_3_long
data_sample_3_long_centered_time$time_centered = data_sample_3_long_centered_time$time - mean(data_sample_3_long_centered_time$time)


mod_rep_int_quad = lmer(pain ~ time_centered + I(time_centered^2) + age + sex + STAI_trait + pain_cat + mindfulness + cortisol_serum + + (1|ID), data = data_sample_3_long_centered_time)

# Now we can request the reportable results the same way we did in the previous exercise. 

# Marginal R squared
r2beta(mod_rep_int_quad, method = "nsj", data = data_sample_3_long_centered_time)

# Conditional AIC
cAIC(mod_rep_int_quad)

# Model coefficients
summary(mod_rep_int_quad)

# Confidence intervals for the coefficients
confint(mod_rep_int_quad)

# standardized Betas
#cannot figure out how to get this, keep getting error?

###MODEL DIAGNOSTICS

library(psych) # for pairs.panels
library(ggplot2) # for ggplot
library(reshape2) # for melt function
library(influence.ME) # for influence (this will also load the lme4 package)
library(lattice) # for qqmath

## Assumptions of linear mixed models

# Remember that the assumptions for linear models were the following:

# * **Normality**: The residuals of the model must be normally distributed
# * **Linearity**: Ther relationship between the outcome variable and the predictor(s) must be linear
# * **Homoscedasticity**: The variance of the residuals are similar at all values of the predictor(s)
# * **No Multicollinearity**: None of the predictors can be linearly determined by the other predictor(s)

#checking for influential outliers

influence_observation = influence(mod_rep_int_quad, obs = T)$alt.fixed # this can take a minute or so
influence_group = influence(mod_rep_int_quad, group = "ID")$alt.fixed

boxplot(influence_observation[,"time_centered"])

boxplot(influence_observation[,"mindfulness"])

boxplot(influence_observation[,"STAI_trait"])

boxplot(influence_observation[, "pain_cat"])
boxplot(influence_observation[, "mindfulness"])
boxplot(influence_observation[, "cortisol_serum"])

# check coefficiants 


par(mfrow=c(1, length(pred_names)))
for(i in 1:length(pred_names)){
  boxplot(influence_observation[,pred_names[i]], main = pred_names[i])
}

for(i in 1:length(pred_names)){
  boxplot(influence_group[,pred_names[i]], main = pred_names[i])
}
par(mfrow=c(1,1))

#Don't see any outliers?

#Normality 

qqmath(mod_rep_int_quad, id = 0.05)

qqmath(ranef(mod_rep_int_quad))




